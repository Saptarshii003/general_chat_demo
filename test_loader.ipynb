{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac5fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python - 3.13.7\n",
    "# This file contains code for parsing, preprocessing, chunking and loading netsuite pdfs in vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002975ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4382d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./Netsuite_pdfs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decaf723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing pdfs, not needed if parsed_docs.json present\n",
    "\n",
    "# loader = DirectoryLoader(\n",
    "#     path=folder_path,\n",
    "#     glob='*.pdf',\n",
    "#     loader_cls=PyPDFLoader\n",
    "# )\n",
    "\n",
    "# docs = loader.lazy_load()\n",
    "\n",
    "# pages = []\n",
    "# async for page in loader.alazy_load():\n",
    "#     pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "455bed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for saving and loading parsed documents\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def save_documents(docs, filename=\"docs.json\"):\n",
    "    # Convert each Document object into a dictionary\n",
    "    # containing its text content and metadata\n",
    "    data = [{\"page_content\": d.page_content, \"metadata\": d.metadata}\n",
    "            for d in docs]\n",
    "\n",
    "    # Save the list of dictionaries into a JSON file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        # ensure_ascii=False keeps Unicode characters readable\n",
    "        # indent=2 makes the JSON human-readable\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def load_documents(filename=\"docs.json\"):\n",
    "    from langchain.schema import Document\n",
    "    import json\n",
    "\n",
    "    # Load the JSON file back into a list of dictionaries\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert dictionaries back into Document objects\n",
    "    return [Document(page_content=d[\"page_content\"], metadata=d[\"metadata\"]) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc15c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not needed if parsed_docs.json present\n",
    "\n",
    "# save_documents(pages, \"parsed_docs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60be926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = load_documents(\"parsed_docs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432530c9",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "457b3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ftfy\n",
    "from cleantext import clean\n",
    "from typing import List, Dict\n",
    "from langchain.schema import Document\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ed150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize PDF text.\n",
    "\n",
    "    Steps:\n",
    "    - Fix broken Unicode characters (using ftfy).\n",
    "    - Clean text using the `clean` function:\n",
    "        * Normalize Unicode\n",
    "        * Remove URLs, emails, phone numbers\n",
    "        * Keep line breaks\n",
    "        * Replace sensitive info with placeholders\n",
    "    - Fix words split across lines with hyphen + newline.\n",
    "    - Collapse multiple spaces/tabs into a single space.\n",
    "    - Limit multiple newlines to at most 2 (preserve paragraphs).\n",
    "    - Normalize quotes and dashes.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw extracted PDF text.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            str: Cleaned text.\n",
    "            bool: True if Unicode was fixed, False otherwise.\n",
    "    \"\"\"\n",
    "    fixed = ftfy.fix_text(text)  # fix broken unicode\n",
    "\n",
    "    # General cleaning (remove URLs, emails, phone numbers, etc.)\n",
    "    text = clean(\n",
    "        fixed,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=False,\n",
    "        lower=False,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_line_breaks=False,            # preserve line breaks\n",
    "        replace_with_email=\"<EMAIL>\",    # replace emails with placeholder\n",
    "        replace_with_phone_number=\"<PHONE>\",  # replace phone numbers\n",
    "        replace_with_url=\"<URL>\",        # replace URLs\n",
    "        lang=\"en\"\n",
    "    )\n",
    "\n",
    "    # Fix words split across lines with a hyphen + newline\n",
    "    text = re.sub(r'-\\s*\\n\\s*', '-', text)\n",
    "\n",
    "    # Collapse multiple spaces/tabs into one space\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    # Normalize multiple newlines -> at most 2 (preserve paragraphs)\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)\n",
    "\n",
    "    # Normalize quotes and dashes for consistency\n",
    "    text = text.replace(\"“\", '\"').replace(\n",
    "        \"”\", '\"').replace(\"’\", \"'\").replace(\"–\", \"-\")\n",
    "\n",
    "    return text.strip(), (fixed != text)  # return also if unicode was fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef40f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprint(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a fingerprint for a given text using SHA256 hashing.\n",
    "    This ensures duplicates are detected across full content.\n",
    "    \"\"\"\n",
    "    # Normalize by lowercasing\n",
    "    normalized = text.lower().encode(\"utf-8\")\n",
    "    # Hash entire content\n",
    "    return hashlib.sha256(normalized).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_documents(pages: List[Document]):\n",
    "    \"\"\"\n",
    "    Clean and validate a list of Document objects.\n",
    "\n",
    "    Steps performed:\n",
    "    - Track statistics (original count, short docs, duplicates, unicode fixes, \n",
    "      character counts before/after cleaning).\n",
    "    - Preprocess each document's text (normalize, clean, fix unicode).\n",
    "    - Count short documents (<50 chars).\n",
    "    - Skip duplicates (based on fingerprint).\n",
    "    - Count how many documents had Unicode fixes.\n",
    "    - Return cleaned documents with updated metadata and stats.\n",
    "\n",
    "    Args:\n",
    "        pages (List[Document]): List of Document objects to clean.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - List[Document]: Cleaned documents with updated metadata.\n",
    "            - dict: Statistics about the cleaning process.\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        \"original_docs\": len(pages),  # total input documents\n",
    "        \"short_docs\": 0,              # count of docs with < 50 characters\n",
    "        \"duplicates\": 0,              # count of removed duplicate docs\n",
    "        \"unicode_fixes\": 0,           # how many docs had unicode issues fixed\n",
    "        \"chars_before\": 0,            # total characters before cleaning\n",
    "        \"chars_after\": 0,             # total characters after cleaning\n",
    "    }\n",
    "\n",
    "    seen = set()         # store fingerprints of processed documents\n",
    "    cleaned_pages = []   # store cleaned Document objects\n",
    "\n",
    "    for page in pages:\n",
    "        orig = page.page_content\n",
    "        stats[\"chars_before\"] += len(orig)  # count original characters\n",
    "\n",
    "        # Clean and normalize the text\n",
    "        cleaned, unicode_fixed = preprocess_text(orig)\n",
    "        stats[\"chars_after\"] += len(cleaned)  # count cleaned characters\n",
    "\n",
    "        # Validation: check if the document is too short\n",
    "        is_short = len(cleaned) < 50\n",
    "        if is_short:\n",
    "            stats[\"short_docs\"] += 1\n",
    "\n",
    "        # Check for duplicates using fingerprint\n",
    "        fp = get_fingerprint(cleaned)\n",
    "        is_duplicate = fp in seen\n",
    "        if is_duplicate:\n",
    "            stats[\"duplicates\"] += 1\n",
    "            continue\n",
    "        else:\n",
    "            seen.add(fp)\n",
    "\n",
    "        # Count Unicode fixes\n",
    "        if unicode_fixed:\n",
    "            stats[\"unicode_fixes\"] += 1\n",
    "\n",
    "        # Create cleaned Document object with updated metadata\n",
    "        cleaned_page = Document(\n",
    "            page_content=cleaned,\n",
    "            metadata={**page.metadata,\n",
    "                      # \"is_short\": is_short,        # optional flags\n",
    "                      # \"is_duplicate\": is_duplicate,\n",
    "                      \"had_unicode_fix\": unicode_fixed}\n",
    "        )\n",
    "        cleaned_pages.append(cleaned_page)\n",
    "\n",
    "    return cleaned_pages, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7ce80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(stats: Dict, pages: List[Document]):\n",
    "    \"\"\"\n",
    "    Print a summary report of the text preprocessing process.\n",
    "\n",
    "    Displays:\n",
    "    - Total number of original documents\n",
    "    - Number of short documents (<50 chars)\n",
    "    - Number of duplicates removed\n",
    "    - Number of documents with Unicode fixes\n",
    "    - Characters reduced (absolute and percentage)\n",
    "    - A sample cleaned text and metadata (if available)\n",
    "\n",
    "    Args:\n",
    "        stats (Dict): Dictionary containing preprocessing statistics \n",
    "                      (produced by `clean_documents`).\n",
    "        pages (List[Document]): List of cleaned Document objects.\n",
    "    \"\"\"\n",
    "    # Header section\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\" TEXT PREPROCESSING REPORT \")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Core statistics\n",
    "    print(f\"Original documents : {stats['original_docs']:,}\")\n",
    "    print(f\"Short docs present : {stats['short_docs']:,}\")\n",
    "    print(f\"Duplicates removed : {stats['duplicates']:,}\")\n",
    "    print(f\"Unicode fixes      : {stats['unicode_fixes']:,}\")\n",
    "\n",
    "    # Calculate and display character reduction\n",
    "    chars_removed = stats[\"chars_before\"] - stats[\"chars_after\"]\n",
    "    reduction_pct = (\n",
    "        chars_removed / stats[\"chars_before\"] * 100) if stats[\"chars_before\"] > 0 else 0\n",
    "    print(f\"Characters reduced : {chars_removed:,} ({reduction_pct:.1f}%)\")\n",
    "\n",
    "    # Show a sample cleaned document (hardcoded index 62 for debugging/demo)\n",
    "    if pages:\n",
    "        print(\"\\nSample cleaned text (with tags):\")\n",
    "        print(f\"\\nText: \\n{pages[62].page_content}\")\n",
    "        print(f\"\\nMetadata: {pages[62].metadata}\")\n",
    "\n",
    "    # Footer\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8467584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pages, stats = clean_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6248f84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      " TEXT PREPROCESSING REPORT \n",
      "========================================\n",
      "Original documents : 32,388\n",
      "Short docs present : 170\n",
      "Duplicates removed : 3,533\n",
      "Unicode fixes      : 28,638\n",
      "Characters reduced : 1,172,109 (1.6%)\n",
      "\n",
      "Sample cleaned text (with tags):\n",
      "\n",
      "Text: \n",
      "Analytics Features\n",
      "32\n",
      "Feature Description\n",
      "KPI Scorecards Add the ability to display a portlet on your dashboard that shows the results of\n",
      "multiple KPIs for multiple date or period ranges. For more information, see the help\n",
      "topic KPI Scorecards\n",
      "Connectivity\n",
      "SuiteAnalytics Connect Enable the SuiteAnalytics Connect feature to access and query your NetSuite data\n",
      "using SQL through database standards such as ODBC, JDBC, and ADO.NET. For\n",
      "more information, see the help topic SuiteAnalytics Connect.\n",
      "NetSuite Analytics Warehouse Configure and transfer data to the NetSuite Analytics Warehouse. You can use any\n",
      "data transferred with Oracle Analytics for applications.\n",
      "Third-party Analytics Integration\n",
      "Tableau®Workbook Export Enable users to export saved search and report results as Tableau®workbooks. For\n",
      "more information, see the help topic Exporting Reports, Searches, and Lists.\n",
      "SuiteAnalytics Workbook\n",
      "SuiteAnalytics Workbook Enable SuiteAnalytics Workbook in your account. After it is enabled, click the\n",
      "Analytics tab in the NetSuite navigation menu to begin using the tool. For more\n",
      "information, see the help topic SuiteAnalytics Workbook Overview.\n",
      "Cached Data in Datasets Select this option to enable using cached data to load your datasets.\n",
      "Intelligent Suite\n",
      "Supply Chain Predicted Risks Receive warnings about potential purchase order risks using the predicted risks\n",
      "portlet with the supply chain control tower.\n",
      "Saved Search\n",
      "HTML Formulas in Search Add HTML to search formulas using Formula (HTML) fields. For more information,\n",
      "see the help topic Evaluating Code in Saved Searches Using Formula(HTML) Fields.\n",
      "Commerce Features\n",
      "Users with the Administrator role can enable the following features on the Web Presence subtab of the\n",
      "Enable Features page.\n",
      "Feature Description\n",
      "Web Site\n",
      "Website Enable initial website functionality, for example, creation of domains. Also enable\n",
      "activation of other Commerce features, such as SuiteCommerce and SuiteCommerce\n",
      "Advanced.\n",
      "Site Builder (Website) Create a Site Builder website to give customers information about your company and\n",
      "enable them to browse your product catalog. For more information, see the help topic\n",
      "Site Builder Overview.\n",
      "Site Builder (Web Store) Add web store capabilities, such as shopping cart and checkout, to Site Builder\n",
      "websites. For more information, see the help topic Site Builder Overview.\n",
      "Advanced Site\n",
      "Customization\n",
      "Use HTML templates to customize the appearance of your website.\n",
      "Account Setup Guide\n",
      "\n",
      "Metadata: {'producer': 'Apache FOP Version 2.4', 'creator': 'PyPDF', 'creationdate': '2025-06-20T11:59:18+02:00', 'title': 'Account Setup Guide', 'author': 'NetSuite', 'source': 'Netsuite_pdfs\\\\AccountSetupGuide.pdf', 'total_pages': 175, 'page': 37, 'page_label': '32', 'had_unicode_fix': True}\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "report(stats, cleaned_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634519e3",
   "metadata": {},
   "source": [
    "## Chunking and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ce6eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk size set due to sentence-transformers/all-mpnet-base-v2 constraints\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1350, chunk_overlap=150)\n",
    "texts = text_splitter.split_documents(cleaned_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ab18b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63103"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "282cd930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_short_chunks(chunks, min_length=50):\n",
    "    \"\"\"\n",
    "    Merge chunks shorter than min_length with previous or next chunk\n",
    "    from the same source PDF.\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return []\n",
    "\n",
    "    merged = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(chunks):\n",
    "        chunk = chunks[i]\n",
    "\n",
    "        if len(chunk.page_content.strip()) < min_length:\n",
    "            merged_flag = False\n",
    "\n",
    "            # Try merge with previous if same PDF\n",
    "            if merged and chunk.metadata.get(\"source\") == merged[-1].metadata.get(\"source\"):\n",
    "                merged[-1].page_content += \" \" + chunk.page_content.strip()\n",
    "                merged_flag = True\n",
    "\n",
    "            # Else, try merge with next if same PDF\n",
    "            elif i + 1 < len(chunks) and chunk.metadata.get(\"source\") == chunks[i + 1].metadata.get(\"source\"):\n",
    "                chunks[i + 1].page_content = chunk.page_content.strip() + \\\n",
    "                    \" \" + chunks[i + 1].page_content\n",
    "                merged_flag = True\n",
    "\n",
    "            # If merged with next, skip adding current\n",
    "            if merged_flag:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        # Add current chunk if not merged\n",
    "        merged.append(chunk)\n",
    "        i += 1\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "843ad581",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = merge_short_chunks(texts, min_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f67e371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62940"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3883d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IDEAPAD\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4487bc",
   "metadata": {},
   "source": [
    "## Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918e0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a local Qdrant client (stored in ./tmp/langchain_qdrant)\n",
    "client = QdrantClient(path=\"./tmp/langchain_qdrant\")\n",
    "\n",
    "# Create a new collection in Qdrant to store embeddings\n",
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",          # name of the collection\n",
    "    vectors_config=VectorParams(\n",
    "        # embedding vector size (depends on model)\n",
    "        size=768,\n",
    "        # similarity metric (COSINE = angular similarity)\n",
    "        distance=Distance.COSINE\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Wrap the Qdrant client in a LangChain VectorStore\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"demo_collection\",          # reference the created collection\n",
    "    embedding=embeddings,                       # embedding function for text\n",
    ")\n",
    "\n",
    "# Add processed text documents to the Qdrant collection\n",
    "qdrant_vector_store.add_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading saved database\n",
    "\n",
    "# client = QdrantClient(path=\"./tmp/langchain_qdrant\")\n",
    "\n",
    "\n",
    "# qdrant_vector_store = QdrantVectorStore(\n",
    "#     client=client,\n",
    "#     collection_name=\"demo_collection\",\n",
    "#     embedding=embeddings,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever using Maximal Marginal Relevance (MMR) search\n",
    "retriever = qdrant_vector_store.as_retriever(\n",
    "    search_type=\"mmr\",                         # search strategy = diversity + relevance\n",
    "    search_kwargs={\n",
    "        \"k\": 10,                               # number of results to return\n",
    "        # diversity vs relevance tradeoff (closer to 1 = more diverse)\n",
    "        \"lambda_mult\": 0.8\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb916ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the retriever to fetch the most relevant document chunks\n",
    "retriever.invoke(\n",
    "    \"How do I set up commission calculations for sales reps?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d48dd4",
   "metadata": {},
   "source": [
    "## Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67598af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a faiss vector store\n",
    "faiss_vector_store = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vectors\n",
    "faiss_vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31776d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever using Maximal Marginal Relevance (MMR) search\n",
    "retriever = faiss_vector_store.as_retriever(\n",
    "    search_type=\"mmr\",                         # search strategy = diversity + relevance\n",
    "    search_kwargs={\n",
    "        \"k\": 10,                               # number of results to return\n",
    "        # diversity vs relevance tradeoff (closer to 1 = more diverse)\n",
    "        \"lambda_mult\": 0.8\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3148a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the retriever to fetch the most relevant document chunks\n",
    "retriever.invoke(\n",
    "    \"How do I set up commission calculations for sales reps?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ee631",
   "metadata": {},
   "source": [
    "## Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chroma vector store\n",
    "chroma_vector_store = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e5a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever using Maximal Marginal Relevance (MMR) search\n",
    "retriever = chroma_vector_store.as_retriever(\n",
    "    search_type=\"mmr\",                         # search strategy = diversity + relevance\n",
    "    search_kwargs={\n",
    "        \"k\": 10,                               # number of results to return\n",
    "        # diversity vs relevance tradeoff (closer to 1 = more diverse)\n",
    "        \"lambda_mult\": 0.8\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c214fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the retriever to fetch the most relevant document chunks\n",
    "retriever.invoke(\n",
    "    \"How do I set up commission calculations for sales reps?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56b803",
   "metadata": {},
   "source": [
    "## Milvus DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URI for storing Milvus data (local SQLite-backed Milvus instance)\n",
    "URI = \"./milvus_example.db\"\n",
    "\n",
    "# Create a Milvus vector store\n",
    "milvus_vector_store = Milvus(\n",
    "    # function to embed documents into vectors\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": URI},              # connection settings for Milvus\n",
    "    index_params={                             # index configuration\n",
    "        \"index_type\": \"FLAT\",                  # FLAT = brute-force search\n",
    "        \"metric_type\": \"L2\"                    # L2 = Euclidean distance\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add processed text documents to the Milvus vector database\n",
    "milvus_vector_store.add_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e3bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever using Maximal Marginal Relevance (MMR) search\n",
    "milvus_retriever = milvus_vector_store.as_retriever(\n",
    "    search_type=\"mmr\",                         # search strategy = diversity + relevance\n",
    "    search_kwargs={\n",
    "        \"k\": 10,                               # number of results to return\n",
    "        # diversity vs relevance tradeoff (closer to 1 = more diverse)\n",
    "        \"lambda_mult\": 0.8\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the retriever to fetch the most relevant document chunks\n",
    "milvus_retriever.invoke(\n",
    "    \"What are the standard and specialized NetSuite centers?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
